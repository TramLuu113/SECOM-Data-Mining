# -*- coding: utf-8 -*-
"""SECOM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Gn0DRWaLdAYMeFOk-zBODIPg7l54HRG_

Group 1 - SECOM Data Mining Project python code
"""

#Install necessary packages for the process
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.impute import KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.ensemble import RandomForestClassifier
!pip install boruta
from boruta import BorutaPy
from imblearn.over_sampling import RandomOverSampler
from sklearn.feature_selection import RFECV
from imblearn.over_sampling import SMOTE as ImbSMOTE
from imblearn.over_sampling import ADASYN as ImbADASYN
from sklearn.model_selection import cross_val_score
from sklearn.svm import SVC
from sklearn.utils import resample
from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score,accuracy_score, precision_score, recall_score, roc_curve
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_validate
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import Lasso, LogisticRegression
from sklearn.feature_selection import SelectFromModel
#!pip install -U imbalanced-learn
from sklearn.feature_selection import RFE
from pprint import pprint
from sklearn.metrics import fbeta_score
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer, fbeta_score

"""Part 1 - Preliminary Data Understanding and treatment


1. Import and merge data sets
2. Treatment on column names
3. Descriptive analysis
4. Distribution of the target variable
5. Stratified train/test split

"""

# Read the orginal raw files into dataframe
secom = pd.read_csv("secom.data", header=None, sep=' ')
secom_labels = pd.read_csv("secom_labels.data", header=None, sep=' ', names=['Pass/Fail', 'timestamp'], quotechar='"')

## Rename column names to "featureX"
secom.columns = ['feature' + str(i) for i in range(len(secom.columns))]

#Merge datasets/ Concatenate the dataframes horizontally
merged_secom = pd.concat([secom, secom_labels], axis=1)
merged_secom = merged_secom.drop(columns = ["timestamp"])

# Summarize descriptive analysis on the entire secom dataset

summary_data= merged_secom.iloc[:,:-2]
descriptives_secom = pd.concat([summary_data.count().rename("count"),
                       summary_data.max().rename("max"),
                       summary_data.min().rename("min"),
                       summary_data.mean().rename("mean"),
                       summary_data.quantile(.25).rename("Q1"),
                       summary_data.quantile(.50).rename("Q2"),
                       summary_data.quantile(.75).rename("Q3"),
                       summary_data.std().rename("standard deviation"),
                       summary_data.var().rename("Variance"),
                       summary_data.isna().sum().rename("total_NA"),
                       (summary_data.isna().sum()/summary_data.count()).rename("%_NA")],axis=1)

# Overview on the distribution of target variable ("Pass"/"Fail")
result_counts = merged_secom['Pass/Fail'].value_counts(normalize=True) * 100
print(result_counts)

plt.bar(result_counts.index, result_counts.values, color= "green")
plt.xticks(result_counts.index)
plt.xlabel('Pass/Fail')
plt.ylabel('Count')
plt.title('Distribution of Pass/Fail')

for i, v in enumerate(result_counts.values):
    plt.text(i, v, f'{v:.1f}%', ha='center', va='bottom', fontweight='bold')

# Set y-axis limits
plt.ylim(0, 100)

plt.show()

# Results: "-1" (good wafers): 93.4 % ; "1" (faulty wafers): 6.6%
# It indicates that balancing is necessary for the training set

#Train/Test split on a 80: 20 ratio
X = merged_secom.drop(columns=["Pass/Fail"])
y = merged_secom["Pass/Fail"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

#Check the percentage of "pass/fail" in the original data and the training/testing sets
print("Original data:")
print(y.value_counts(normalize=True))
print("Training data:")
print(y_train.value_counts(normalize=True))
print("Test data:")
print(y_test.value_counts(normalize=True))

print("Training set size:", len(X_train))
print("Test set size:", len(X_test))

# We get the result of Training set size: 1253; Test set size: 314
# Distribution of the target variable in both training set and test set remain the same as in the original dataset

"""Part 2 - Preprocess the traning set


1.   Descriptive analysis
2.   Rough dimensionality reduction ( based on constant features and threshold of missing value percentage)
3.   Outliers detection and treatment



"""

#Summarize descriptive analysis on the training set
descriptive_train = pd.concat([X_train.count().rename("count"),
                       X_train.max().rename("max"),
                       X_train.min().rename("min"),
                       X_train.mean().rename("mean"),
                       X_train.quantile(.25).rename("Q1"),
                       X_train.quantile(.50).rename("Q2"),
                       X_train.quantile(.75).rename("Q3"),
                       X_train.std().rename("standard deviation"),
                       X_train.var().rename("Variance"),
                       X_train.isna().sum().rename("total_NA"),
                       X_train.isna().sum()/X_train.count().rename("%_NA")],axis=1)

#Build correalation matrix and heatmap
corr_matrix = X_train.corr()

plt.figure(figsize=(20,20))
sns.heatmap(corr_matrix, cmap='coolwarm', center=0, annot=True)
plt.show()

#Select only the unique correlation pairs
total_corr = corr_matrix.shape[0] * (corr_matrix.shape[0] - 1) / 2
print(total_corr)


#Correlation percentage for each category
perfect_corr = (np.abs(corr_matrix) == 1).sum().sum() / 2 / total_corr * 100

print(perfect_corr) #0.14%

strong_corr = ((np.abs(corr_matrix) >= 0.8) & (np.abs(corr_matrix) < 1)).sum().sum() / 2 / total_corr * 100
print(strong_corr) #0.39%

mod_corr = ((np.abs(corr_matrix) >= 0.5) & (np.abs(corr_matrix) < 0.8)).sum().sum() / 2 / total_corr * 100
print(mod_corr) #0.43%

wk_corr = ((np.abs(corr_matrix) >= 0.3) & (np.abs(corr_matrix) < 0.5)).sum().sum() / 2 / total_corr * 100
print(wk_corr) #0.46%

no_corr = (np.abs(corr_matrix) < 0.3).sum().sum() / 2 / total_corr * 100
print(no_corr) #63.13%

# Create a list of the values
values = [no_corr, wk_corr, mod_corr, strong_corr, perfect_corr]

# Create a list of the labels
labels = ['No_Corr', 'Weak_Corr', 'Moderate_Corr', 'Strong_Corr', 'Perfect_Corr']

# Create a bar chart
plt.bar(labels, values,color="green")

# Add percentage labels on bars
for i in range(len(values)):
    plt.text(i, values[i]+0.5, '{:.2f}%'.format(values[i]), ha='center', va='bottom', fontweight='bold')

# Set the title and y-axis label
plt.title('Correlation Distribution')
plt.ylabel('Percentage of Features')

# Rotate x-axis labels
plt.xticks(rotation=45)

# Display the plot
plt.show()

# Identify constant features -  variance = 0
# calculate variance of each feature
variance = X_train.var()
variance_df = pd.DataFrame(variance, columns=['variance'])

# plot a histogram of the variance of training set
plt.hist(variance_df['variance'], bins=50, color="green")
plt.xlabel('Variance')
plt.ylabel('Frequency')
plt.title('Distribution of Feature Variances')
plt.show()

# Identify constant features where variance = 0
constant_feature = variance[variance==0].index.tolist()
num_constant_features = (variance==0).sum()
# 116 constant features found

# Identify duplicated rows
duplicated_rows = X_train.duplicated()
print(duplicated_rows.sum()) # 0 rows found

# Identify duplicate columns
duplicated_cols = X_train.transpose().duplicated()
print(duplicated_cols.sum()) #104 columns found

#Drop constant features (the duplicated columns are overlapped with the constant columns)
constant_columns = X_train.columns[X_train.nunique() == 1]
X_train.drop(constant_columns, axis=1, inplace=True)

#The features reduced to 474

# Define the percentage of missing values contained in each column
na_percent = X_train.isnull().mean()*100
print(na_percent)

#Histogram of missing values
plt.hist(na_percent)
plt.xlabel('Percentage of Missing Value')
plt.ylabel('Count of Features')
plt.title('Histogram of Variance for All Features')
plt.show()

#Count columns that has over 50% missing values
na_threshold = 50
missing_50 = sum(na_percent > na_threshold)
print(missing_50)
# 24 found

# Drop the columns with missing values over 50%
na_to_drop = na_percent[na_percent > na_threshold].index
X_train = X_train.drop(columns= na_to_drop)

# The features reduced to 450

#Identify outliers
z_scores= (X_train-X_train.mean())/X_train.std()
outliers= (z_scores>3)|(z_scores<-3)
num_outliers= outliers.sum()
print(num_outliers)

#Replace outliers with 3s boundries
lower_boundary = X_train.mean() - 3 * X_train.std()
upper_boundary = X_train.mean() + 3 * X_train.std()
X_train[outliers] = np.where(z_scores>3, upper_boundary, lower_boundary)

"""Part 3 - Define functions to later automate different pre-processing combinations


1. Missing Values imputation (KNN, MICE)
2. Feature Selection (Boruta, LASSO, Recursive)
3. Balancing (ROSE, SMOTE, ADASYN)
4. Scaling - Normalization


"""

# Missing values imputation functions
## Method 1 - KNN

def knn_imputation_with_scaling(df, n_neighbors=5):
    # Create a copy of the dataframe
    df_processed = df.copy()

    # Select the numeric columns for scaling
    numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns

    # Perform scaling
    scaler = StandardScaler()
    df_processed[numeric_columns] = scaler.fit_transform(df_processed[numeric_columns])

    # Perform KNN imputation
    imputer = KNNImputer(n_neighbors=n_neighbors)
    df_processed = pd.DataFrame(imputer.fit_transform(df_processed), columns=df_processed.columns)

    # Reverse the scaling
    df_processed[numeric_columns] = scaler.inverse_transform(df_processed[numeric_columns])

    return df_processed

# Missing values imputation functions
## Method 2 - MICE
def mice_imputation(df):
    # Create MICE imputer objects
    MICE_imputer = IterativeImputer()

    # MICE imputation
    MICE_imputed_df = MICE_imputer.fit_transform(df)

    # Convert the data back to a DataFrame and retain the original column names
    MICE_df = pd.DataFrame(MICE_imputed_df, columns=df.columns)


    return MICE_df

# Feature Selection functions
## Method 1 - Boruta
def Boruta_FS(df_x, df_y):
    # Separate features and target variable
    X = df_x.values
    y = df_y.values

    # Boruta feature selection
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    boruta_selector = BorutaPy(rf, n_estimators=200, max_iter=100, verbose=2, random_state=42)

    boruta_selector.fit(X, y)
    selected_features_df = pd.DataFrame({'Feature': df_x.columns.tolist(), 'Ranking': boruta_selector.ranking_, 'Support': boruta_selector.support_, 'Tentative': boruta_selector.support_weak_})
    selected_features_1 = selected_features_df[selected_features_df['Support'] == True]['Feature'] # this is the confirmed features
    selected_features_2 = selected_features_df[selected_features_df['Tentative'] == True]['Feature'] # this is the tentative features
    Bo_confirmed = df_x[selected_features_1]
    Bo_tentative = df_x[selected_features_2]
    X_selected = pd.concat([Bo_confirmed, Bo_tentative], axis=1)

    Xy_selected = pd.concat([X_selected, df_y], axis=1)


    return Xy_selected

# Feature Selection functions
## Method 2 - LASSO
def Lasso_FS(df, target_column, alpha):

    # Separate X (features) and y (target variable)
    X = df.drop(target_column, axis=1)
    y = df[target_column]

    # Create a Lasso model with the specified alpha
    lasso = Lasso(alpha=alpha)

    # Fit the Lasso model on the data
    lasso.fit(X, y)

    # Get the coefficients
    coef = lasso.coef_

    # Get the indices of non-zero coefficients
    selected_features = [i for i, c in enumerate(coef) if c != 0]

    # Select the final features from X
    selected_X = X.iloc[:, selected_features]

    # Combine the selected features with y into the final dataframe
    selected_df = pd.concat([selected_X, y], axis=1)

    return selected_df

# Feature Selection functions
## Method 3 - RECURSIVE
def Recursive_FS(df):
    # Define X and Y
    RE_X = df.drop('Pass/Fail', axis=1)
    RE_Y = df['Pass/Fail']

    # Define the estimator (Random Forest Classifier)
    Re_RF_Class = RandomForestClassifier(n_estimators=100, random_state=42)

    # Perform Recursive Feature Elimination with Cross Validation
    Re_RFE = RFE(estimator=Re_RF_Class, n_features_to_select=25, step=10)

    # Fit the selector on the data
    Re_RFE.fit(RE_X, RE_Y)

    # Get the selected feature mask
    Re_RFE_feature_mask = Re_RFE.support_

    # Filter the features in the original dataset based on the selected mask
    Re_selected = pd.DataFrame(RE_X.loc[:, Re_RFE_feature_mask], columns=RE_X.columns[Re_RFE_feature_mask])

    # Combine the selected features and the target variable
    Re_selected_full_df = pd.concat([Re_selected, RE_Y], axis=1)

    return Re_selected_full_df

# Balancing
## Method 1 - ROSE
def ROSE_balance(df):

    # define x & y
    Rose_X = df.drop('Pass/Fail',axis=1)
    Rose_Y = df['Pass/Fail']


    # define desired sampling strategy / parameters
    rose = RandomOverSampler(sampling_strategy='auto', random_state = 11)

    # fit
    Rose_resam_X, Rose_resam_Y = rose.fit_resample(Rose_X, Rose_Y)

    # Join X and y to see the Pass/Fail ratio in the balanced dataset
    Rose_XY = pd.concat([Rose_resam_X,Rose_resam_Y], axis=1)
    Rose_XY_ratio = Rose_XY['Pass/Fail'].value_counts(normalize=True)* 100
    print(Rose_XY_ratio)

    return Rose_XY

# Balancing
## Method 2 - SMOTE
def SMOTE_balance(df):

    # define X & Y
    SMO_X = df.drop('Pass/Fail', axis=1)
    SMO_Y = df['Pass/Fail']

    # define desired sampling strategy / parameters
    smote = ImbSMOTE(sampling_strategy='auto', random_state = 11)

    # fit
    Smo_resam_X, Smo_resam_Y = smote.fit_resample(SMO_X, SMO_Y)

    # Join X and y to see the Pass/Fail ratio in the balanced dataset
    Smote_XY = pd.concat([Smo_resam_X,Smo_resam_Y], axis=1)
    Smote_XY_ratio = Smote_XY['Pass/Fail'].value_counts(normalize=True)* 100
    print(Smote_XY_ratio)

    return Smote_XY

# Balancing
## Method 3 - ADASYN
def ADASYN_balance(df):

    # define X & Y
    AD_X = df.drop('Pass/Fail', axis=1)
    AD_Y = df['Pass/Fail']

    # define desired sampling strategy / parameters
    adasyn = ImbADASYN(sampling_strategy='auto', random_state = 11)

    # fit
    AD_resam_X, AD_resam_Y = adasyn.fit_resample(AD_X, AD_Y)

    # Join X and y to see the Pass/Fail ratio in the balanced dataset
    AD_XY = pd.concat([AD_resam_X,AD_resam_Y], axis=1)
    AD_XY_ratio = AD_XY['Pass/Fail'].value_counts(normalize=True)* 100
    print(AD_XY_ratio)

    return AD_XY

# Scaling - define a scaling function for models that required scaling
### Scaling (input- x only)

def normalize(df_x):

    scaler = MinMaxScaler()
    normalized_data = scaler.fit_transform(df_x)
    normalized_df  = pd.DataFrame(normalized_data, columns=df_x.columns)

    return normalized_df

"""Part 4 - Model training and Evaluation

Selected models: SVM Model/ Naive Bayers/ Random Forest

- Preprocess on training set: rouch dimensionality reduction - outliers treatment - missing values imputation - feature selections - normalization(for only SVM &NB)

- Preporcess on test set: outliers treatment - Missing values imputation - Align selected features - Normalization(for only SVM &NB)


1. Combination 1: KNN - BORUTA - ROSE
2. Combination 2: KNN - BORUTA - SMOTE
3. Combination 3: KNN - BORUTA - ADASYN
4. Combination 4: KNN - LASSO - ROSE
5. Combination 5: KNN - LASSO - SMOTE
6. Combination 6: KNN - LASSO - ADASYN
7. Combination 7: KNN - RECURSIVE - ROSE
8. Combination 8: KNN - RECURSIVE - SMOTE
9. Combination 9: KNN - RECURSIVE - ADASYN
10. Combination 10:MICE - BORUTA - ROSE
2. Combination 11: MICE - BORUTA - SMOTE
3. Combination 12: MICE - BORUTA - ADASYN
4. Combination 13: MICE - LASSO - ROSE
5. Combination 14: MICE - LASSO - SMOTE
6. Combination 15: MICE - LASSO - ADASYN
7. Combination 16: MICE - RECURSIVE - ROSE
8. Combination 17: MICE - RECURSIVE - SMOTE
9. Combination 18: MICE - RECURSIVE - ADASYN


"""

## Define the function to fit the test set on the trained model and perform evalutation
def evaluate_model(model, X_train, y_train, X_test, y_test):

    # Fit the model to the training data
    model.fit(X_train, y_train)

    # Make predictions on the test data
    y_pred = model.predict(X_test)

    # Calculate accuracy score
    accuracy = accuracy_score(y_test, y_pred)

  # Calculate confusion matrix
    cm = confusion_matrix(y_test, y_pred)

    # Calculate precision score
    precision = precision_score(y_test, y_pred)

    # Calculate recall score
    recall = recall_score(y_test, y_pred)

    # Calculate true negatives (TN)
    TN = cm[0, 0]

    # Calculate false positives (FP)
    FP = cm[0, 1]

    # Calculate false negatives (FN)
    FN = cm[1, 0]

    # Calculate true positives (TP)
    TP = cm[1, 1]

    # Calculate F1 score
    f1 = f1_score(y_test, y_pred)

    # Calculate F2 score
    f2 = fbeta_score(y_test, y_pred, beta=2)

    # Calculate AUC score
    auc = roc_auc_score(y_test, y_pred)

    # Generate ROC curve
    fpr, tpr, thresholds = roc_curve(y_test, y_pred)

    # Plot ROC curve
    plt.plot(fpr, tpr)
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.show()

    # Plot confusion matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', center=0.5)
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')

    # Set x-axis and y-axis tick labels
    plt.xticks([0.5, 1.5], ['Pass', 'Fail'])
    plt.yticks([0.5, 1.5], ['Pass', 'Fail'])

    plt.show()

    # Return evaluation results
    evaluation_results = {
        'Accuracy': accuracy,
        'Confusion Matrix': cm,
        'Precision': precision,
        'Recall': recall,
        'F1 Score': f1,
        'F2 Score': f2,
        'AUC Score': auc,
        'TN': TN,
        'FP': FP,
        'FN': FN,
        'TP': TP
    }

    print(evaluation_results)

"""Part 4.1
Support Vector Machine Model Training and Evalutation
"""

# SVM Combination 1: KNN - BORUTA - ROSE
## Preprocess the training set - 10 features are selected
df_na = pd.concat([X_train,y_train],axis=1)
knn_imputed_SVM1 = knn_imputation_with_scaling(df_na,n_neighbors=5)

Bo_X = knn_imputed_SVM1.drop(columns=["Pass/Fail"])
Bo_y = knn_imputed_SVM1["Pass/Fail"]
Bo_sel_SVM1 = Boruta_FS(Bo_X,Bo_y)

train_pre_1 = ROSE_balance(Bo_sel_SVM1)
train_final_1 = normalize(train_pre_1)

# Preprocess the test set
## Identify outliers - test set
z_scores_test= (X_test-X_test.mean())/X_test.std()
outliers_test= (z_scores_test>3)|(z_scores_test<-3)
num_outliers_test= outliers_test.sum()

## Replace outliers with 3s boundries -test set
lower_boundary_test = X_test.mean() - 3 * X_test.std()
upper_boundary_test = X_test.mean() + 3 * X_test.std()
X_test[outliers_test] = np.where(z_scores_test>3, upper_boundary_test, lower_boundary_test)

## KNN imputation with test set
df_na_test = pd.concat([X_test,y_test],axis=1)
knn_imputed_SVM1_test = knn_imputation_with_scaling(df_na_test,n_neighbors=5)

## Slice the same Boruta selected features in training set for test set
Bo_selected_features = np.array(Bo_sel_SVM1.columns)
test_pre_1 = knn_imputed_SVM1_test[Bo_selected_features]

## Scaling on the test set
test_final_1 = normalize(test_pre_1)

# Evaluation on the SVM1 model
X_train_SVM1 = train_final_1.drop(columns = ["Pass/Fail"])
y_train_SVM1 = train_final_1 ["Pass/Fail"]
X_test_SVM1 = test_final_1.drop(columns = ["Pass/Fail"])
y_test_SVM1 = test_final_1 ["Pass/Fail"]
SVM1_result = evaluate_model(SVC(), X_train_SVM1, y_train_SVM1, X_test_SVM1,y_test_SVM1)
print(SVM1_result)

# SVM Combination 2: KNN - BORUTA - SMOTE
## Preprocess the training set
train_pre_2 = SMOTE_balance(Bo_sel_SVM1)
train_final_2 = normalize(train_pre_2)

## Preprocess the test set (same test set to apply as in the SVM1)

#Evaluation on the SVM2 model
X_train_SVM2 = train_final_2.drop(columns = ["Pass/Fail"])
y_train_SVM2 = train_final_2 ["Pass/Fail"]
X_test_SVM1 = test_final_1.drop(columns = ["Pass/Fail"])
y_test_SVM1 = test_final_1 ["Pass/Fail"]
SVM2_result = evaluate_model(SVC(), X_train_SVM2, y_train_SVM2, X_test_SVM1,y_test_SVM1)
print(SVM2_result)

# SVM Combination 3:KNN - BORUTA - ADASYN
## Preprocess the training set
train_pre_3 = ADASYN_balance(Bo_sel_SVM1)
train_final_3 = normalize(train_pre_3)

## Preprocess the test set (same test set to apply as in the SVM1)

# Evaluation on the SVM3 model
X_train_SVM3 = train_final_3.drop(columns = ["Pass/Fail"])
y_train_SVM3 = train_final_3 ["Pass/Fail"]
X_test_SVM1 = test_final_1.drop(columns = ["Pass/Fail"])
y_test_SVM1 = test_final_1 ["Pass/Fail"]
SVM3_result = evaluate_model(SVC(), X_train_SVM3, y_train_SVM3, X_test_SVM1,y_test_SVM1)
print(SVM3_result)

# SVM Combination 4: KNN - LASSO - ROSE
## Preprocess the training set - 19 features are selected
La_sel_SVM4 = Lasso_FS(knn_imputed_SVM1,"Pass/Fail",3)
train_pre_4 = ROSE_balance(La_sel_SVM4)
train_final_4 = normalize(train_pre_4)

# Slice same features for test set
La_selected_features = np.array(La_sel_SVM4.columns)
test_pre_4 = knn_imputed_SVM1_test[La_selected_features]

## Scaling on the test set
test_final_4 = normalize(test_pre_4)

# Evaluation on the SVM4 model
X_train_SVM4 = train_final_4.drop(columns = ["Pass/Fail"])
y_train_SVM4 = train_final_4 ["Pass/Fail"]
X_test_SVM4 = test_final_4.drop(columns = ["Pass/Fail"])
y_test_SVM4 = test_final_4 ["Pass/Fail"]
SVM4_result = evaluate_model(SVC(), X_train_SVM4, y_train_SVM4, X_test_SVM4,y_test_SVM4)
print(SVM4_result)

# SVM Combination 5: KNN - LASSO - SMOTE
## Preprocess the training set
train_pre_5 = SMOTE_balance(La_sel_SVM4)
train_final_5 = normalize(train_pre_5)

## Preprocess the test set (same test set to apply as in the SVM4)

# Evaluation on the SVM5 model
X_train_SVM5 = train_final_5.drop(columns = ["Pass/Fail"])
y_train_SVM5 = train_final_5 ["Pass/Fail"]
X_test_SVM5 = test_final_4.drop(columns = ["Pass/Fail"])
y_test_SVM5 = test_final_4 ["Pass/Fail"]
SVM5_result = evaluate_model(SVC(), X_train_SVM5, y_train_SVM5, X_test_SVM5,y_test_SVM5)
print(SVM5_result)

# SVM Combination 6: KNN - LASSO - ADASYN
## Preprocess the training set
train_pre_6 = ADASYN_balance(La_sel_SVM4)
train_final_6 = normalize(train_pre_6)

## Preprocess the test set (same test set to apply as in the SVM4)

# Evaluation on the SVM6 model
X_train_SVM6 = train_final_6.drop(columns = ["Pass/Fail"])
y_train_SVM6 = train_final_6 ["Pass/Fail"]
X_test_SVM6 = test_final_4.drop(columns = ["Pass/Fail"])
y_test_SVM6 = test_final_4 ["Pass/Fail"]
SVM6_result = evaluate_model(SVC(), X_train_SVM6, y_train_SVM6, X_test_SVM6,y_test_SVM6)
print(SVM6_result)

# SVM Combination 7: KNN - RECURSIVE - ROSE
## Preprocess the training set - 25 features are selected
Re_sel_SVM7 = Recursive_FS(knn_imputed_SVM1)
train_pre_7 = ROSE_balance(Re_sel_SVM7)
train_final_7 = normalize(train_pre_7)

# Slice same features for test set
Re_selected_features = np.array(Re_sel_SVM7.columns)
test_pre_7 = knn_imputed_SVM1_test[Re_selected_features]

## Scaling on the test set
test_final_7 = normalize(test_pre_7)

# Evaluation on the SVM7 model
X_train_SVM7 = train_final_7.drop(columns = ["Pass/Fail"])
y_train_SVM7 = train_final_7 ["Pass/Fail"]
X_test_SVM7 = test_final_7.drop(columns = ["Pass/Fail"])
y_test_SVM7 = test_final_7 ["Pass/Fail"]
SVM7_result = evaluate_model(SVC(), X_train_SVM7, y_train_SVM7, X_test_SVM7,y_test_SVM7)
print(SVM7_result)

# SVM Combination 8: KNN - RECURSIVE - SMOTE
## Preprocess the training set
train_pre_8 = SMOTE_balance(Re_sel_SVM7)
train_final_8 = normalize(train_pre_8)

## Preprocess the test set (same test set to apply as in the SVM7)

# Evaluation on the SVM8 model
X_train_SVM8 = train_final_8.drop(columns = ["Pass/Fail"])
y_train_SVM8 = train_final_8 ["Pass/Fail"]
X_test_SVM8 = test_final_7.drop(columns = ["Pass/Fail"])
y_test_SVM8 = test_final_7 ["Pass/Fail"]
SVM8_result = evaluate_model(SVC(), X_train_SVM8, y_train_SVM8, X_test_SVM8,y_test_SVM8)
print(SVM8_result)

# SVM Combination 9: KNN - RECURSIVE - ADASYN
## Preprocess the training set
train_pre_9 = ADASYN_balance(Re_sel_SVM7)
train_final_9 = normalize(train_pre_9)

## Preprocess the test set (same test set to apply as in the SVM7)

# Evaluation on the SVM9 model
X_train_SVM9 = train_final_9.drop(columns = ["Pass/Fail"])
y_train_SVM9 = train_final_9 ["Pass/Fail"]
X_test_SVM9 = test_final_7.drop(columns = ["Pass/Fail"])
y_test_SVM9 = test_final_7 ["Pass/Fail"]
SVM9_result = evaluate_model(SVC(), X_train_SVM9, y_train_SVM9, X_test_SVM9,y_test_SVM9)
print(SVM9_result)

# SVM Combination 10: MICE - BORUTA - ROSE
## Preprocess the training set - 10 features are selected
df_na = pd.concat([X_train,y_train],axis=1)
mice_imputed_SVM = mice_imputation(df_na)
Bo_X = mice_imputed_SVM.drop(columns=["Pass/Fail"])
Bo_y = mice_imputed_SVM["Pass/Fail"]
Bo_sel_SVM10 = Boruta_FS(Bo_X,Bo_y)
train_pre_10 = ROSE_balance(Bo_sel_SVM10)
train_final_10 = normalize(train_pre_10)

## Preprocess the test set

### MICE imputation with test set
df_na_test = pd.concat([X_test,y_test],axis=1)
mice_imputed_SVM_test = mice_imputation(df_na_test)

### Slice the same Boruta selected features in training set for test set
Bo_selected_features = np.array(Bo_sel_SVM10.columns)
test_pre_10 = mice_imputed_SVM_test[Bo_selected_features]

### Scaling on the test set
test_final_10 = normalize(test_pre_10)

##Evaluation on the SVM10 model
X_train_SVM10 = train_final_10.drop(columns = ["Pass/Fail"])
y_train_SVM10 = train_final_10 ["Pass/Fail"]
X_test_SVM10 = test_final_10.drop(columns = ["Pass/Fail"])
y_test_SVM10 = test_final_10 ["Pass/Fail"]
SVM10_result = evaluate_model(SVC(), X_train_SVM10, y_train_SVM10, X_test_SVM10,y_test_SVM10)
print(SVM10_result)

# SVM Combination 11: MICE - BORUTA - SMOTE
##Preprocess the training set - 10 features are selected
train_pre_11 = SMOTE_balance(Bo_sel_SVM10)
train_final_11 = normalize(train_pre_11)

## Preprocess the test set (same test set to apply as in the SVM10)

##Evaluation on the SVM11 model
X_train_SVM11 = train_final_11.drop(columns = ["Pass/Fail"])
y_train_SVM11 = train_final_11 ["Pass/Fail"]
X_test_SVM11 = test_final_10.drop(columns = ["Pass/Fail"])
y_test_SVM11 = test_final_10 ["Pass/Fail"]
SVM11_result = evaluate_model(SVC(), X_train_SVM11, y_train_SVM11, X_test_SVM11,y_test_SVM11)
print(SVM11_result)

# SVM Combination 12: MICE - BORUTA - ADASYN
##Preprocess the training set - 10 features are selected
train_pre_12 = ADASYN_balance(Bo_sel_SVM10)
train_final_12 = normalize(train_pre_12)

## Preprocess the test set (same test set to apply as in the SVM10)

##Evaluation on the SVM12 model
X_train_SVM12 = train_final_12.drop(columns = ["Pass/Fail"])
y_train_SVM12 = train_final_12 ["Pass/Fail"]
X_test_SVM12 = test_final_10.drop(columns = ["Pass/Fail"])
y_test_SVM12 = test_final_10 ["Pass/Fail"]
SVM12_result = evaluate_model(SVC(), X_train_SVM12, y_train_SVM12, X_test_SVM12,y_test_SVM12)
print(SVM12_result)

# SVM Combination 13: MICE - LASSO - ROSE
##Preprocess the training set - 19 features are selected

La_X = mice_imputed_SVM.drop(columns=["Pass/Fail"])
La_y = mice_imputed_SVM["Pass/Fail"]
La_sel_SVM13 = Lasso_FS(La_X,La_y)
train_pre_13 = ROSE_balance(La_sel_SVM13)
train_final_13 = normalize(train_pre_13)

## Preprocess the test set

### Slice the same Lasso selected features in training set for test set
La_selected_features = np.array(La_sel_SVM13.columns)
test_pre_13 = mice_imputed_SVM_test[La_selected_features]

### Scaling on the test set
test_final_13 = normalize(test_pre_13)

## Evaluation on the SVM10 model
X_train_SVM13 = train_final_13.drop(columns = ["Pass/Fail"])
y_train_SVM13 = train_final_13 ["Pass/Fail"]
X_test_SVM13 = test_final_13.drop(columns = ["Pass/Fail"])
y_test_SVM13 = test_final_13 ["Pass/Fail"]
SVM13_result = evaluate_model(SVC(), X_train_SVM13, y_train_SVM13, X_test_SVM13,y_test_SVM13)
print(SVM13_result)

# SVM Combination 14: MICE - LASSO - SMOTE
##Preprocess the training set - 10 features are selected

train_pre_14 = OSE_balance(La_sel_SVM13)
train_final_14 = normalize(train_pre_14)

## Preprocess the test set (same test set to apply as in the SVM13)

##Evaluation on the SVM14 model
X_train_SVM14 = train_final_14.drop(columns = ["Pass/Fail"])
y_train_SVM14 = train_final_14 ["Pass/Fail"]
X_test_SVM14 = test_final_13.drop(columns = ["Pass/Fail"])
y_test_SVM14 = test_final_13 ["Pass/Fail"]
SVM14_result = evaluate_model(SVC(), X_train_SVM14, y_train_SVM14, X_test_SVM14,y_test_SVM14)
print(SVM14_result)

# SVM Combination 15: MICE - LASSO - ADASYN
##Preprocess the training set - 19 features are selected

train_pre_15 = ADASYN_balance(La_sel_SVM13)
train_final_15 = normalize(train_pre_15)

## Preprocess the test set (same test set to apply as in the SVM13)

##Evaluation on the SVM14 model
X_train_SVM15 = train_final_15.drop(columns = ["Pass/Fail"])
y_train_SVM15 = train_final_15 ["Pass/Fail"]
X_test_SVM15 = test_final_13.drop(columns = ["Pass/Fail"])
y_test_SVM15 = test_final_13 ["Pass/Fail"]
SVM15_result = evaluate_model(SVC(), X_train_SVM15, y_train_SVM15, X_test_SVM15,y_test_SVM15)
print(SVM15_result)

# SVM Combination 16: MICE - RECURSIVE - ROSE
##Preprocess the training set - 25 features are selected

Re_sel_SVM16 = Recursive_FS(mice_imputed_SVM)
train_pre_16 = ROSE_balance(Re_sel_SVM16)
train_final_16 = normalize(train_pre_16)

## Preprocess the test set

### Slice the same Recursive selected features in training set for test set
Re_selected_features = np.array(Re_sel_SVM16.columns)
test_pre_16 = mice_imputed_SVM_test[Re_selected_features]

### Scaling on the test set
test_final_16 = normalize(test_pre_16)

##Evaluation on the SVM10 model
X_train_SVM16 = train_final_16.drop(columns = ["Pass/Fail"])
y_train_SVM16 = train_final_16 ["Pass/Fail"]
X_test_SVM16 = test_final_16.drop(columns = ["Pass/Fail"])
y_test_SVM16 = test_final_16 ["Pass/Fail"]
SVM16_result = evaluate_model(SVC(), X_train_SVM16, y_train_SVM16, X_test_SVM16,y_test_SVM16)
print(SVM16_result)

# SVM Combination 17: MICE - RECURSIVE - SMOTE
##Preprocess the training set - 25 features are selected

train_pre_17 = ROSE_balance(Re_sel_SVM16)
train_final_17 = normalize(train_pre_17)

## Preprocess the test set (same test set to apply as in the SVM16)

##Evaluation on the SVM17 model
X_train_SVM17 = train_final_17.drop(columns = ["Pass/Fail"])
y_train_SVM17 = train_final_17 ["Pass/Fail"]
X_test_SVM17 = test_final_16.drop(columns = ["Pass/Fail"])
y_test_SVM17 = test_final_16 ["Pass/Fail"]
SVM17_result = evaluate_model(SVC(), X_train_SVM17, y_train_SVM17, X_test_SVM17,y_test_SVM17)
print(SVM17_result)

# SVM Combination 18: MICE - RECURSIVE - ADASYN
##Preprocess the training set - 25 features are selected

train_pre_18 = ROSE_balance(Re_sel_SVM16)
train_final_18 = normalize(train_pre_18)

## Preprocess the test set (same test set to apply as in the SVM16)

##Evaluation on the SVM17 model
X_train_SVM18 = train_final_18.drop(columns = ["Pass/Fail"])
y_train_SVM18 = train_final_18 ["Pass/Fail"]
X_test_SVM18 = test_final_16.drop(columns = ["Pass/Fail"])
y_test_SVM18 = test_final_16 ["Pass/Fail"]
SVM18_result = evaluate_model(SVC(), X_train_SVM18, y_train_SVM18, X_test_SVM18,y_test_SVM18)
print(SVM18_result)

"""Part 4.2 Naïve Bayes classifier Model building and Evalutation"""

# NB Combination 1: KNN - BORUTA - ROSE
## *Preprocessed traning and test set will are based on combination 1*

#Evaluation on the NB1 model
X_train_NB1 = train_final_1.drop(columns = ["Pass/Fail"])
y_train_NB1 = train_final_1 ["Pass/Fail"]
X_test_NB1 = test_final_1.drop(columns = ["Pass/Fail"])
y_test_NB1 = test_final_1 ["Pass/Fail"]
NB1_result = evaluate_model(GaussianNB(), X_train_NB1, y_train_NB1, X_test_NB1,y_test_NB1)
print(NB1_result)

# NB Combination 2: KNN - BORUTA - SMOTE
## *Preprocessed traning and test set are based on combination 2*

#Evaluation on the NB2 model
X_train_NB2 = train_final_2.drop(columns = ["Pass/Fail"])
y_train_NB2 = train_final_2 ["Pass/Fail"]
X_test_NB2 = test_final_1.drop(columns = ["Pass/Fail"])
y_test_NB2 = test_final_1 ["Pass/Fail"]
NB2_result = evaluate_model(GaussianNB(), X_train_NB2, y_train_NB2, X_test_NB2,y_test_NB2)
print(NB2_result)

# NB Combination 3: KNN - BORUTA - ADASYN
## *Preprocessed traning and test set are based on combination 3*

#Evaluation on the NB3 model
X_train_NB3 = train_final_3.drop(columns = ["Pass/Fail"])
y_train_NB3 = train_final_3 ["Pass/Fail"]
X_test_NB3 = test_final_1.drop(columns = ["Pass/Fail"])
y_test_NB3 = test_final_1 ["Pass/Fail"]
NB3_result = evaluate_model(GaussianNB(), X_train_NB1, y_train_NB1, X_test_NB1,y_test_NB1)
print(NB3_result)

# NB Combination 4: KNN - LASSO - ROSE
## *Preprocessed traning and test set are based on combination 4*

#Evaluation on the NB4 model
X_train_NB4 = train_final_4.drop(columns = ["Pass/Fail"])
y_train_NB4 = train_final_4 ["Pass/Fail"]
X_test_NB4 = test_final_4.drop(columns = ["Pass/Fail"])
y_test_NB4 = test_final_4 ["Pass/Fail"]
NB4_result = evaluate_model(GaussianNB(), X_train_NB4, y_train_NB4, X_test_NB4,y_test_NB4)
print(NB4_result)

# NB Combination 5: KNN - LASSO - SMOTE
## *Preprocessed traning and test set are based on combination 5*

#Evaluation on the NB5 model
X_train_NB5 = train_final_5.drop(columns = ["Pass/Fail"])
y_train_NB5 = train_final_5 ["Pass/Fail"]
X_test_NB5 = test_final_4.drop(columns = ["Pass/Fail"])
y_test_NB5 = test_final_4 ["Pass/Fail"]
NB5_result = evaluate_model(GaussianNB(), X_train_NB5, y_train_NB5, X_test_NB5,y_test_NB5)
print(NB5_result)

# NB Combination 6: KNN - LASSO - ADASYN
## *Preprocessed traning and test set are based on combination 6*

#Evaluation on the NB6 model
X_train_NB6 = train_final_6.drop(columns = ["Pass/Fail"])
y_train_NB6 = train_final_6 ["Pass/Fail"]
X_test_NB6 = test_final_4.drop(columns = ["Pass/Fail"])
y_test_NB6 = test_final_4 ["Pass/Fail"]
NB6_result = evaluate_model(GaussianNB(), X_train_NB6, y_train_NB6, X_test_NB6,y_test_NB6)
print(NB6_result)

# NB Combination 7: KNN - RECURSIVE - ROSE
## *Preprocessed traning and test set are based on combination 7*

#Evaluation on the NB7 model
X_train_NB7 = train_final_7.drop(columns = ["Pass/Fail"])
y_train_NB7 = train_final_7 ["Pass/Fail"]
X_test_NB7 = test_final_7.drop(columns = ["Pass/Fail"])
y_test_NB7 = test_final_7 ["Pass/Fail"]
NB7_result = evaluate_model(GaussianNB(), X_train_NB7, y_train_NB7, X_test_NB7,y_test_NB7)
print(NB7_result)

# NB Combination 8: KNN - RECURSIVE - SMOTE
## *Preprocessed traning and test set are based on combination 8*

#Evaluation on the NB8 model
X_train_NB8 = train_final_8.drop(columns = ["Pass/Fail"])
y_train_NB8 = train_final_8 ["Pass/Fail"]
X_test_NB8 = test_final_7.drop(columns = ["Pass/Fail"])
y_test_NB8 = test_final_7 ["Pass/Fail"]
NB8_result = evaluate_model(GaussianNB(), X_train_NB8, y_train_NB8, X_test_NB8,y_test_NB8)
print(NB8_result)

# NB Combination 9: KNN - RECURSIVE - ADASYN
## *Preprocessed traning and test set are based on combination 9*

#Evaluation on the NB9 model
X_train_NB9 = train_final_9.drop(columns = ["Pass/Fail"])
y_train_NB9 = train_final_9 ["Pass/Fail"]
X_test_NB9 = test_final_7.drop(columns = ["Pass/Fail"])
y_test_NB9 = test_final_7 ["Pass/Fail"]
NB9_result = evaluate_model(GaussianNB(), X_train_NB9, y_train_NB9, X_test_NB9,y_test_NB9)
print(NB9_result)

# NB Combination 10: MICE - BORUTA - ROSE
## *Preprocessed traning and test set are based on combination 10*

#Evaluation on the NB10 model
X_train_NB10 = train_final_10.drop(columns = ["Pass/Fail"])
y_train_NB10 = train_final_10 ["Pass/Fail"]
X_test_NB10 = test_final_10.drop(columns = ["Pass/Fail"])
y_test_NB10 = test_final_10 ["Pass/Fail"]
NB10_result = evaluate_model(GaussianNB(), X_train_NB10, y_train_NB10, X_test_NB10,y_test_NB10)
print(NB10_result)

# NB Combination 11: MICE - BORUTA - SMOTE
## *Preprocessed traning and test set are based on combination 11*

#Evaluation on the NB11 model
X_train_NB11 = train_final_11.drop(columns = ["Pass/Fail"])
y_train_NB11 = train_final_11 ["Pass/Fail"]
X_test_NB11 = test_final_10.drop(columns = ["Pass/Fail"])
y_test_NB11 = test_final_10 ["Pass/Fail"]
NB11_result = evaluate_model(GaussianNB(), X_train_NB11, y_train_NB11, X_test_NB11,y_test_NB11)
print(NB11_result)

# NB Combination 12: MICE - BORUTA - ADASYN
## *Preprocessed traning and test set are based on combination 12*

#Evaluation on the NB12 model
X_train_NB12 = train_final_12.drop(columns = ["Pass/Fail"])
y_train_NB12 = train_final_12 ["Pass/Fail"]
X_test_NB12 = test_final_10.drop(columns = ["Pass/Fail"])
y_test_NB12 = test_final_10 ["Pass/Fail"]
NB12_result = evaluate_model(GaussianNB(), X_train_NB12, y_train_NB12, X_test_NB12,y_test_NB12)
print(NB12_result)

# NB Combination 13: MICE - LASSO- ROSE
## *Preprocessed traning and test set are based on combination 13*

#Evaluation on the NB13 model
X_train_NB13 = train_final_13.drop(columns = ["Pass/Fail"])
y_train_NB13 = train_final_13 ["Pass/Fail"]
X_test_NB13 = test_final_13.drop(columns = ["Pass/Fail"])
y_test_NB13 = test_final_13 ["Pass/Fail"]
NB13_result = evaluate_model(GaussianNB(), X_train_NB13, y_train_NB13, X_test_NB13,y_test_NB13)
print(NB13_result)

# NB Combination 14: MICE - LASSO- SMOTE
## *Preprocessed traning and test set are based on combination 14*

#Evaluation on the NB14 model
X_train_NB14 = train_final_14.drop(columns = ["Pass/Fail"])
y_train_NB14 = train_final_14 ["Pass/Fail"]
X_test_NB14 = test_final_13.drop(columns = ["Pass/Fail"])
y_test_NB14 = test_final_13 ["Pass/Fail"]
NB14_result = evaluate_model(GaussianNB(), X_train_NB14, y_train_NB14, X_test_NB14,y_test_NB14)
print(NB14_result)

# NB Combination 15: MICE - LASSO- ADASYN
## *Preprocessed traning and test set are based on combination 15*

#Evaluation on the NB15 model
X_train_NB15 = train_final_15.drop(columns = ["Pass/Fail"])
y_train_NB15 = train_final_15 ["Pass/Fail"]
X_test_NB15 = test_final_13.drop(columns = ["Pass/Fail"])
y_test_NB15 = test_final_13 ["Pass/Fail"]
NB15_result = evaluate_model(GaussianNB(), X_train_NB15, y_train_NB15, X_test_NB15,y_test_NB15)
print(NB15_result)

# NB Combination 16: MICE - RECURSIVE- ROSE
## *Preprocessed traning and test set are based on combination 16*

#Evaluation on the NB16 model
X_train_NB16 = train_final_16.drop(columns = ["Pass/Fail"])
y_train_NB16 = train_final_16 ["Pass/Fail"]
X_test_NB16 = test_final_16.drop(columns = ["Pass/Fail"])
y_test_NB16 = test_final_16 ["Pass/Fail"]
NB16_result = evaluate_model(GaussianNB(), X_train_NB16, y_train_NB16, X_test_NB16,y_test_NB16)
print(NB16_result)

# NB Combination 17: MICE - RECURSIVE - SMOTE
## *Preprocessed traning and test set are based on combination 17*

#Evaluation on the NB17 model
X_train_NB17 = train_final_17.drop(columns = ["Pass/Fail"])
y_train_NB17 = train_final_17 ["Pass/Fail"]
X_test_NB17 = test_final_16.drop(columns = ["Pass/Fail"])
y_test_NB17 = test_final_16 ["Pass/Fail"]
NB17_result = evaluate_model(GaussianNB(), X_train_NB17, y_train_NB17, X_test_NB17,y_test_NB17)
print(NB17_result)

# NB Combination 18: MICE - RECURSIVE - ADASYN
## *Preprocessed traning and test set are based on combination 18*

#Evaluation on the NB18 model
X_train_NB18 = train_final_18.drop(columns = ["Pass/Fail"])
y_train_NB18 = train_final_18 ["Pass/Fail"]
X_test_NB18 = test_final_16.drop(columns = ["Pass/Fail"])
y_test_NB18 = test_final_16 ["Pass/Fail"]
NB18_result = evaluate_model(GaussianNB(), X_train_NB18, y_train_NB18, X_test_NB18,y_test_NB18)
print(NB18_result)

"""Part 4.3 Random Forest Model building and Evalutation (No scaling performed on the preprocessed training and test set)"""

#Define the function to train RF model and get evaluation metrics

def evaluate_RF_model(X_train, y_train, X_test, y_test, n_estimators=100, random_state=42):

    # Convert -1(pass) to negative and 1(Fail) to positive in y_train and y_test
    y_train.replace({-1:0, 1:1})
    y_test.replace({-1:0, 1:1})


    # Instantiate Random Forest model
    model = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)

    # Fit the model to the training data
    model.fit(X_train, y_train)

    # Make predictions on the test data
    y_pred = model.predict(X_test)

    # Calculate accuracy score
    accuracy = accuracy_score(y_test, y_pred)

    # Calculate confusion matrix
    cm = confusion_matrix(y_test, y_pred)

    # Calculate precision score
    precision = precision_score(y_test, y_pred)

    # Calculate recall score
    recall = recall_score(y_test, y_pred)

    # Calculate true negatives (TN)
    TN = cm[0, 0]

    # Calculate false positives (FP)
    FP = cm[0, 1]

    # Calculate false negatives (FN)
    FN = cm[1, 0]

    # Calculate true positives (TP)
    TP = cm[1, 1]

    # Calculate F1 score
    f1 = f1_score(y_test, y_pred)

    # Calculate F2 score
    f2 = fbeta_score(y_test, y_pred, beta=2)

    # Calculate AUC score
    auc = roc_auc_score(y_test, y_pred)

    # Generate ROC curve
    fpr, tpr, thresholds = roc_curve(y_test, y_pred)

    # Plot ROC curve
    plt.plot(fpr, tpr)
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.show()

    # Plot confusion matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', center=0.5)
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')

    # Set x-axis and y-axis tick labels
    plt.xticks([0.5, 1.5], ['Pass', 'Fail'])
    plt.yticks([0.5, 1.5], ['Pass', 'Fail'])

    plt.show()

    # Return evaluation results
    evaluation_results = {
        'Accuracy': accuracy,
        'Confusion Matrix': cm,
        'Precision': precision,
        'Recall': recall,
        'F1 Score': f1,
        'F2 Score': f2,
        'AUC Score': auc,
        'TN': TN,
        'FP': FP,
        'FN': FN,
        'TP': TP
    }

    print(evaluation_results)

# RF Combination 1: KNN - BORUTA - ROSE
## *Preprocessed traning and test set will are based on combination 1 (without scaling)*

#Evaluation on the RF1 model
X_train_RF1 = train_pre_1.drop(columns = ["Pass/Fail"])
y_train_RF1 = train_pre_1 ["Pass/Fail"]
X_test_RF1 = test_pre_1.drop(columns = ["Pass/Fail"])
y_test_RF1 = test_pre_1 ["Pass/Fail"]
RF1_result = evaluate_RF_model(X_train_RF1, y_train_RF1, X_test_RF1,y_test_RF1,n_estimators=100, random_state=42)
print(RF1_result)

# RF Combination 2: KNN - BORUTA - SMOTE
## *Preprocessed traning and test set will are based on combination 2 (without scaling)*

#Evaluation on the RF2 model
X_train_RF2 = train_pre_2.drop(columns = ["Pass/Fail"])
y_train_RF2 = train_pre_2 ["Pass/Fail"]
X_test_RF2 = test_pre_1.drop(columns = ["Pass/Fail"])
y_test_RF2 = test_pre_1 ["Pass/Fail"]
RF2_result = evaluate_RF_model(X_train_RF2, y_train_RF2, X_test_RF2,y_test_RF2,n_estimators=100, random_state=42)
print(RF2_result)

# RF Combination 3: KNN - BORUTA - ADASYN
## *Preprocessed traning and test set will are based on combination 3 (without scaling)*

#Evaluation on the RF3 model
X_train_RF3 = train_pre_3.drop(columns = ["Pass/Fail"])
y_train_RF3 = train_pre_3 ["Pass/Fail"]
X_test_RF3 = test_pre_1.drop(columns = ["Pass/Fail"])
y_test_RF3 = test_pre_1 ["Pass/Fail"]
RF3_result = evaluate_RF_model(X_train_RF3, y_train_RF3, X_test_RF3,y_test_RF3,n_estimators=100, random_state=42)
print(RF3_result)

# RF Combination 4: KNN - LASSO - ROSE
## *Preprocessed traning and test set will are based on combination4 (without scaling)*

#Evaluation on the RF4 model
X_train_RF4 = train_pre_4.drop(columns = ["Pass/Fail"])
y_train_RF4 = train_pre_4 ["Pass/Fail"]
X_test_RF4 = test_pre_4.drop(columns = ["Pass/Fail"])
y_test_RF4 = test_pre_4 ["Pass/Fail"]
RF4_result = evaluate_RF_model(X_train_RF4, y_train_RF4, X_test_RF4,y_test_RF4,n_estimators=100, random_state=42)
print(RF4_result)

# RF Combination 5: KNN - LASSO - SMOTE
## *Preprocessed traning and test set will are based on combination 5 (without scaling)*

#Evaluation on the RF5 model
X_train_RF5 = train_pre_5.drop(columns = ["Pass/Fail"])
y_train_RF5 = train_pre_5 ["Pass/Fail"]
X_test_RF5 = test_pre_4.drop(columns = ["Pass/Fail"])
y_test_RF5 = test_pre_4 ["Pass/Fail"]
RF5_result = evaluate_RF_model(X_train_RF5, y_train_RF5, X_test_RF5,y_test_RF5,n_estimators=100, random_state=42)
print(RF5_result)

# RF Combination 6: KNN - LASSO - ADASYN
## *Preprocessed traning and test set will are based on combination 6 (without scaling)*

#Evaluation on the RF6 model
X_train_RF6 = train_pre_6.drop(columns = ["Pass/Fail"])
y_train_RF6 = train_pre_6 ["Pass/Fail"]
X_test_RF6 = test_pre_4.drop(columns = ["Pass/Fail"])
y_test_RF6 = test_pre_4 ["Pass/Fail"]
RF6_result = evaluate_RF_model(X_train_RF6, y_train_RF6, X_test_RF6,y_test_RF6,n_estimators=100, random_state=42)
print(RF6_result)

# RF Combination 7: KNN - RECURSIVE - ROSE
## *Preprocessed traning and test set will are based on combination 7 (without scaling)*

#Evaluation on the RF7 model
X_train_RF7 = train_pre_7.drop(columns = ["Pass/Fail"])
y_train_RF7 = train_pre_7 ["Pass/Fail"]
X_test_RF7 = test_pre_7.drop(columns = ["Pass/Fail"])
y_test_RF7 = test_pre_7 ["Pass/Fail"]
RF7_result = evaluate_RF_model(X_train_RF7, y_train_RF7, X_test_RF7,y_test_RF7,n_estimators=100, random_state=42)
print(RF7_result)

# RF Combination 8: KNN - RECURSIVE - SMOTE
## *Preprocessed traning and test set will are based on combination 8 (without scaling)*

#Evaluation on the RF8 model
X_train_RF8 = train_pre_8.drop(columns = ["Pass/Fail"])
y_train_RF8 = train_pre_8 ["Pass/Fail"]
X_test_RF8 = test_pre_7.drop(columns = ["Pass/Fail"])
y_test_RF8 = test_pre_7 ["Pass/Fail"]
RF8_result = evaluate_RF_model(X_train_RF8, y_train_RF8, X_test_RF8,y_test_RF8,n_estimators=100, random_state=42)
print(RF8_result)

# RF Combination 9: KNN - RECURSIVE - ADASYN
## *Preprocessed traning and test set will are based on combination 9 (without scaling)*

#Evaluation on the RF9 model
X_train_RF9 = train_pre_9.drop(columns = ["Pass/Fail"])
y_train_RF9 = train_pre_9["Pass/Fail"]
X_test_RF9 = test_pre_7.drop(columns = ["Pass/Fail"])
y_test_RF9 = test_pre_7 ["Pass/Fail"]
RF9_result = evaluate_RF_model(X_train_RF9, y_train_RF9, X_test_RF9,y_test_RF9,n_estimators=100, random_state=42)
print(RF9_result)

# RF Combination 10: MICE - BORUTA - ROSE
## *Preprocessed traning and test set will are based on combination 10 (without scaling)*

#Evaluation on the RF10 model
X_train_RF10 = train_pre_10.drop(columns = ["Pass/Fail"])
y_train_RF10 = train_pre_10 ["Pass/Fail"]
X_test_RF10 = test_pre_10.drop(columns = ["Pass/Fail"])
y_test_RF10 = test_pre_10 ["Pass/Fail"]
RF10_result = evaluate_RF_model(X_train_RF10, y_train_RF10, X_test_RF10,y_test_RF10,n_estimators=100, random_state=42)
print(RF10_result)

# RF Combination 11: MICE - BORUTA - SMOTE
## *Preprocessed traning and test set will are based on combination 11 (without scaling)*

#Evaluation on the RF11 model
X_train_RF11 = train_pre_11.drop(columns = ["Pass/Fail"])
y_train_RF11 = train_pre_11 ["Pass/Fail"]
X_test_RF11 = test_pre_10.drop(columns = ["Pass/Fail"])
y_test_RF11 = test_pre_10 ["Pass/Fail"]
RF11_result = evaluate_RF_model(X_train_RF11, y_train_RF11, X_test_RF11,y_test_RF11,n_estimators=100, random_state=42)
print(RF11_result)

# RF Combination 12: MICE - BORUTA - ADASYN
## *Preprocessed traning and test set will are based on combination 12 (without scaling)*

#Evaluation on the RF12 model
X_train_RF12 = train_pre_12.drop(columns = ["Pass/Fail"])
y_train_RF12 = train_pre_12 ["Pass/Fail"]
X_test_RF12 = test_pre_10.drop(columns = ["Pass/Fail"])
y_test_RF12 = test_pre_10 ["Pass/Fail"]
RF12_result = evaluate_RF_model(X_train_RF12, y_train_RF12, X_test_RF12,y_test_RF12,n_estimators=100, random_state=42)
print(RF12_result)

# RF Combination 13: MICE - LASSO - ROSE
## *Preprocessed traning and test set will are based on combination 13 (without scaling)*

#Evaluation on the RF13 model
X_train_RF13 = train_pre_13.drop(columns = ["Pass/Fail"])
y_train_RF13 = train_pre_13 ["Pass/Fail"]
X_test_RF13 = test_pre_13.drop(columns = ["Pass/Fail"])
y_test_RF13 = test_pre_13 ["Pass/Fail"]
RF13_result = evaluate_RF_model(X_train_RF13, y_train_RF13, X_test_RF13,y_test_RF13,n_estimators=100, random_state=42)
print(RF13_result)

# RF Combination 14: MICE - LASSO - SMOTE
## *Preprocessed traning and test set will are based on combination 14 (without scaling)*

#Evaluation on the RF14 model
X_train_RF14 = train_pre_14.drop(columns = ["Pass/Fail"])
y_train_RF14 = train_pre_14 ["Pass/Fail"]
X_test_RF14 = test_pre_13.drop(columns = ["Pass/Fail"])
y_test_RF14 = test_pre_13 ["Pass/Fail"]
RF14_result = evaluate_RF_model(X_train_RF14, y_train_RF14, X_test_RF14,y_test_RF14,n_estimators=100, random_state=42)
print(RF14_result)

# RF Combination 15: MICE - LASSO - ADASYN
## *Preprocessed traning and test set will are based on combination 15 (without scaling)*

#Evaluation on the RF15 model
X_train_RF15 = train_pre_15.drop(columns = ["Pass/Fail"])
y_train_RF15 = train_pre_15 ["Pass/Fail"]
X_test_RF15 = test_pre_13.drop(columns = ["Pass/Fail"])
y_test_RF15 = test_pre_13 ["Pass/Fail"]
RF15_result = evaluate_RF_model(X_train_RF15, y_train_RF15, X_test_RF15,y_test_RF15,n_estimators=100, random_state=42)
print(RF15_result)

# RF Combination 16: MICE - RECURSIVE - ROSE
## *Preprocessed traning and test set will are based on combination 16 (without scaling)*

#Evaluation on the RF16 model
X_train_RF16 = train_pre_16.drop(columns = ["Pass/Fail"])
y_train_RF16 = train_pre_16 ["Pass/Fail"]
X_test_RF16 = test_pre_16.drop(columns = ["Pass/Fail"])
y_test_RF16 = test_pre_16 ["Pass/Fail"]
RF16_result = evaluate_RF_model(X_train_RF16, y_train_RF16, X_test_RF16,y_test_RF16,n_estimators=100, random_state=42)
print(RF16_result)

# RF Combination 17: MICE - RECURSIVE - SMOTE
## *Preprocessed traning and test set will are based on combination 17 (without scaling)*

#Evaluation on the RF17 model
X_train_RF17 = train_pre_17.drop(columns = ["Pass/Fail"])
y_train_RF17 = train_pre_17 ["Pass/Fail"]
X_test_RF17 = test_pre_16.drop(columns = ["Pass/Fail"])
y_test_RF17 = test_pre_16 ["Pass/Fail"]
RF17_result = evaluate_RF_model(X_train_RF17, y_train_RF17, X_test_RF17,y_test_RF17,n_estimators=100, random_state=42)
print(RF17_result)

# RF Combination 18: MICE - RECURSIVE - ADASYN
## *Preprocessed traning and test set will are based on combination 18 (without scaling)*

#Evaluation on the RF18 model
X_train_RF18 = train_pre_18.drop(columns = ["Pass/Fail"])
y_train_RF18 = train_pre_18 ["Pass/Fail"]
X_test_RF18 = test_pre_16.drop(columns = ["Pass/Fail"])
y_test_RF18 = test_pre_16 ["Pass/Fail"]
RF18_result = evaluate_RF_model(X_train_RF18, y_train_RF18, X_test_RF18,y_test_RF18,n_estimators=100, random_state=42)
print(RF18_result)

"""Part 5 - Select the Best Model
- Based on the thorough evalution on the 7 scores, 5 models with best performance and predictive power were selected (filtered by the ranking of AUC, Recall, F2 score)
- Conduct grid search on the selected models to fine-tune parameters and explore the possibility to enhance model performance
- Generate Confusion Matrix for the final top 5 models to facilitate cost calculation

Top 5 Models:
1. SVM7 (KNN-RECURSIVE- ROSE)
2. SVM10 (MICE - BORUTA -ROSE)
3. NB1 (KNN-BORUTA-ROSE)
4. NB2 (KNN-BORUTA-SMOTE)
5. NB12 (MICE - BORUTA - ADASYN)
"""

# Define the function to perform grid search

def perform_grid_search(model, param_grid, X_train, y_train):

    # define the F2
    scorer = make_scorer(fbeta_score, beta=2)

    # Create a GridSearchCV object
    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring=scorer)

    # Fit the Grid Search to the training data
    grid_search.fit(X_train, y_train)

    # Get the best model from the Grid Search
    best_model = grid_search.best_estimator_

    # Print the best parameters and the corresponding score
    print("Best Parameters:", grid_search.best_params_)
    print("Best Score:", grid_search.best_score_)

    return best_model

# Define the parameter grid for SVM
svm_param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf'],
    'gamma': ['scale', 'auto'],
    'shrinking': [True, False],
    'probability': [True, False]
}


# Create an SVM model
svm_model = SVC()

# Perform Grid Search for SVM7 model
best_svm7_model = perform_grid_search(svm_model, svm_param_grid, X_train_SVM7, y_train_SVM7)

SVM7_result_GS = evaluate_model(SVC(C=10, gamma='scale', kernel='rbf',shrinking=True,probability=True), X_train_SVM7, y_train_SVM7, X_test_SVM7,y_test_SVM7)
print(SVM7_result_GS)
# ****Results worse than the original SVM7 model**

# Perform Grid Search for SVM7 model
best_svm10_model = perform_grid_search(svm_model, svm_param_grid, X_train_SVM10, y_train_SVM10)

SVM10_result_GS = evaluate_model(SVC(C=10, gamma='scale', kernel='rbf',shrinking=True,probability=True), X_train_SVM10, y_train_SVM10, X_test_SVM10,y_test_SVM10)
print(SVM10_result_GS)
# ****Results worse than the original SVM10 model**

# Define the parameter grid for Naive Bayes
nb_param_grid = {
    'var_smoothing': [1e-10, 1e-9, 1e-8],
}

# Create a Naive Bayes model
nb_model = GaussianNB()

# Perform Grid Search for NB1 model
best_nb1_model = perform_grid_search(nb_model, nb_param_grid, X_train_NB1, y_train_NB1)

# Evaluation on the NB1 model with best parameters
NB1_result_GS = evaluate_model(GaussianNB(var_smoothing=1e-10), X_train_NB1, y_train_NB1, X_test_NB1,y_test_NB1)
print(NB1_result_GS)

# **Results are the same with the original NB1 model**

# Perform Grid Search for NB2 model
best_nb2_model = perform_grid_search(nb_model, nb_param_grid, X_train_NB2, y_train_NB2)

# Evaluation on the NB1 model with best parameters
NB2_result_GS = evaluate_model(GaussianNB(var_smoothing=1e-10), X_train_NB2, y_train_NB2, X_test_NB2,y_test_NB2)
print(NB2_result_GS)
# **Results are the same with the original NB2 model**

# Perform Grid Search for NB12 model
best_nb12_model = perform_grid_search(nb_model, nb_param_grid, X_train_NB12, y_train_NB12)

# Evaluation on the NB1 model with best parameters
NB12_result_GS = evaluate_model(GaussianNB(var_smoothing=1e-10), X_train_NB12, y_train_NB12, X_test_NB12,y_test_NB12)
print(NB12_result_GS)

# ****Results worse than the original NB12 model**

"""# END
After exploring grid search, we decided to go with the original parameters for the Top5 selected model to proceed the cost calucation and further define the best model.
It will be reflected in the presentation slides.
"""